{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open in [nbviewer](http://nbviewer.jupyter.org/github/luiarthur/GLM_AMS274/blob/master/notes/notes09.ipynb)\n",
    "$\n",
    "% Latex definitions\n",
    "% note: Ctrl-shfit-p for shortcuts menu\n",
    "\\newcommand{\\iid}{\\overset{iid}{\\sim}}\n",
    "\\newcommand{\\ind}{\\overset{ind}{\\sim}}\n",
    "\\newcommand{\\p}[1]{\\left(#1\\right)}\n",
    "\\newcommand{\\bk}[1]{\\left[#1\\right]}\n",
    "\\newcommand{\\bc}[1]{ \\left\\{#1\\right\\} }\n",
    "\\newcommand{\\abs}[1]{ \\left|#1\\right| }\n",
    "\\newcommand{\\norm}[1]{ \\left|\\left|#1\\right|\\right| }\n",
    "\\newcommand{\\E}{ \\text{E} }\n",
    "\\newcommand{\\N}{ \\mathcal N }\n",
    "\\newcommand{\\ds}{ \\displaystyle }\n",
    "\\newcommand{\\R}{ \\mathbb{R} }\n",
    "\\newcommand{\\suml}{ \\sum_{i=1}^n }\n",
    "\\newcommand{\\prodl}{ \\prod_{i=1}^n }\n",
    "\\newcommand{\\overunderset}[3]{\\overset{#1}{\\underset{#2}{#3}}}\n",
    "\\newcommand{\\asym}{\\overset{\\cdot}{\\sim}}\n",
    "\\newcommand{\\given}{\\bigg |}\n",
    "\\newcommand{\\M}{\\mathcal{M}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Model Comparison (Bayesian GLM)\n",
    "- CPO\n",
    "  - point estimates only? more unstable at predictions around 0?\n",
    "- Bayes Factors\n",
    "- measure using CPOs\n",
    "- posterior predictive criterion (Gelfand 1990)\n",
    "\n",
    "## Bayes Factors (operates on Prior Predictive space)\n",
    "Two models $\\M_0, \\M_1$ (not necessarily nested).  Given prior probs $P(\\M_i)$, then $P(\\M_i|data)$ can be computed, for $i=1,...,I$. And models can be compared. Mind that this assumes that there are only $I$ models.\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "BF_{01} &= \\frac{P(\\M_1|data)/P(\\M_0|data)}{P(\\M_1)/P(\\M_0)} \\\\\n",
    "        &= \\frac{P(data|\\M_1)}{P(data|\\M_0)} \\\\\n",
    "\\end{split}\n",
    "$$\n",
    "which is the ratio of prior predictives. Note $P(data|\\M_i) = \\ds\\int f(data|\\theta_i) p(\\theta_i) d\\theta_i$, where $\\theta_i$ are the (possibly vector of) parameters.\n",
    "\n",
    "see also:\n",
    "- model averaging with finite number of models\n",
    "- BNP {Raftery (1996)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPO\n",
    "\n",
    "- log pseudo-Bayes factor\n",
    "\n",
    "Review CPO in notes08.\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "LS_{CV} &= \\frac{1}{n} \\suml \\log \\bc{P_{CV}^{\\M_j}(y_i|x_i, data_{-i}) } \\\\\n",
    "        &= \\frac{1}{n} \\suml \\log CPO_i \\\\\n",
    "\\end{split}\n",
    "$$\n",
    "where $y_i$ is the **actual** observation and $CPO_i$ is the posterior predictive cv density for $z_i$ at evaluated at $y_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior Predictive \n",
    "- Ibrahim 7 Land (1994)\n",
    "- Gelfand & Ghosh (1998)\n",
    "- IC&S(2001)\n",
    "- CD&I(2004)\n",
    "\n",
    "### Quadratic Loss Measure\n",
    "$n$ is sample size. $\\mathcal{L}(\\M_j) = \\suml var^{\\M_j}(z_i|data) + \\frac{K}{K+1}\\suml\\bk{y_i-\\E^{\\M_j}\\p{z_i|data}}^2$, where $z_i$ is the replicate response corresponding to the $y_i$ given the observed $x_i$, $p(z_i|data)$. i.e. $z_i$ is the posterior predictive, not the CPO, etc.\n",
    "\n",
    "This is just penalty term + goodness of fit term, kinda like DIC. Avoids the need to count number of parameters. $K$ is a constant, chosen by the practitioner. Big $K$ means you put equal weight on penalty and goodness terms. Small $K$ means you only put weight on the penalty term.\n",
    "\n",
    "## Loss Function\n",
    "$\\mathcal{L}(z,a;y) = (z-a)^T(z-a) + K(y-a)^T(y-a)$, fixed $K>0$, chosen by practitioner. Data is $y$, replicate responses are $z$.  Favor models that minimize.\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\underset{a}{\\min}\\E\\p{\\mathcal{L}(z,a;y)}\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "$y,z,a$ are $n$ vectors\n",
    "\n",
    "Under quadratic loss, \n",
    "$$\n",
    "\\begin{split}\n",
    "\\E\\bk{\\mathcal{L}(z,a;y)} &= K\\suml(y_i-a)^2 + \\suml\\E\\bk{(z_i-a_i)^2} \\\\\n",
    "&= \\suml\\bc{Var(z_i|y) + (E(z_i|y)-a_i)^2 + K(y_i-a_i)^2}\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "The $a_i$ that minimizes the expression above is $a_i=\\frac{E(z_i|y) + Ky_i}{K+1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For GLMs, \n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "E\\bk{z_i|data} &= \\int g^{-1}(x_i^T\\beta) p(\\beta|data) d\\beta \\\\\n",
    "&\\approx \\frac{1}{J} \\sum_{i=1}^J g^{-1}(x_i^T\\beta^*) \\\\\n",
    "E\\bk{z_i^2|data} &= \\int \\int z_i^2 EDF(z_i|\\beta,\\phi,x_i) dz_i p(\\beta,\\phi|data) d\\beta d\\phi \\\\\n",
    "&\\approx \\int \\bc{\\phi b''(\\theta_i) + g^{-1}(x_i^T\\beta)^2} p(\\beta,\\phi|data) d\\beta d\\phi \\\\\n",
    "\\end{split}\n",
    "$$\n",
    "We can then compute V($z_i | data$) as $\\E\\bk{z_i^2|data} + \\E\\bk{z_i|data}^2$. Alternately, you can just compute these quantities in the usual MCMC manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{split}\n",
    "y_i | \\pi_i &\\sim Bin(m_i,\\pi) \\\\\n",
    "\\phi &= 1 \\\\\n",
    "b(\\theta_i) &= m_i \\log(1+e^{\\theta_i}) \\\\\n",
    "\\E(y_i|\\pi_i) = b'(\\theta_i) &= m_i\\pi_i \\\\\n",
    "\\theta_i &= logit(\\pi_i) \\\\\n",
    "\\end{split}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.5",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
